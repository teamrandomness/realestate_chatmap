{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8624fa73-80c4-4e7f-a8b8-c43ff2647df0",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install typing-extensions\n",
    "%pip install openai\n",
    "%pip install sqlparse==0.5.0\n",
    "%pip install mlflow>=2.9.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38feafd-0ba6-4378-9d45-da2320d7f9b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trying to register to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52805a8-b650-4417-9635-3f5255934ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow[databricks]\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429cd64c-63a6-4d96-8d01-81cc97239543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel, ModelSignature\n",
    "from mlflow.types import DataType, Schema\n",
    "import mlflow.pyfunc\n",
    "import mlflow.deployments\n",
    "import os\n",
    "\n",
    "def get_table_schema(table_name, spark_object):\n",
    "    table_schema = spark_object.sql(\"DESCRIBE {}\".format(table_name))\n",
    "    my_schema = table_schema.collect()\n",
    "    return (table_name, my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be69b332-ea6e-48df-9b23-90d0156b7b12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom model class for TxtToSQL\n",
    "class TxtToSQLModel(PythonModel):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def nl_to_sql(self, client, nl_query, schema):\n",
    "        chat_response = client.predict(\n",
    "            endpoint=\"databricks-meta-llama-3-70b-instruct\",\n",
    "            inputs={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an AI assistant\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Table schema:\\n{schema}\\n\\nConvert the following natural language query to SQL: {nl_query}\\n\\nSQL: and in the output give only the SQL query without text\",\n",
    "                    },\n",
    "                ],\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 256,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return chat_response.choices[0]\n",
    "\n",
    "    # Method for loading the model\n",
    "    def load_context(self, context):\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    # Method for predicting using the loaded model\n",
    "    def predict(self, model_input):\n",
    "        # Custom prediction logic goes here\n",
    "        pass\n",
    "    \"\"\"\n",
    "    def predict(self, client, message, table_name, spark_object):\n",
    "        table_schema = get_table_schema(table_name, spark_object)\n",
    "        sql_query = self.nl_to_sql(client, message, table_schema)\n",
    "        response = sql_query[\"message\"][\"content\"]\n",
    "        response = response.replace(\"```\", \"\")\n",
    "        #df = spark_object.sql(response)\n",
    "        return response\n",
    "\n",
    "# Create an instance of the custom model\n",
    "txtToSQLModel = TxtToSQLModel()\n",
    "\n",
    "# Model Signature to be added to MLlow registration\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"message\"),\n",
    "    ColSpec(\"string\", \"table_name\")\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"sql_query\")\n",
    "])\n",
    "model_signature = ModelSignature(\n",
    "    inputs=input_schema,\n",
    "    outputs=output_schema\n",
    ")\n",
    "\n",
    "# Log the model with MLflow\n",
    "mlflow.pyfunc.log_model(\n",
    "    \"my_custom_model\",\n",
    "    python_model=txtToSQLModel,\n",
    "    artifacts={},\n",
    "    signature=model_signature,\n",
    "    registered_model_name=\"workspace.default.txt_to_sql_llama3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f679ef-e932-4a05-aeec-15aaf74172d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom model class for TxtToSQL\n",
    "class DefineTxtQuery(PythonModel):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def nl_to_sql(self, client, nl_query, schema):\n",
    "        chat_response = client.predict(\n",
    "            endpoint=\"databricks-meta-llama-3-70b-instruct\",\n",
    "            inputs={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an AI assistant\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"From the table having schema :\\n{schema}\\n\\nWhat would you look at to answer this question : {nl_query}\\n\\n Give an answer that contains a description of how you would analyze this\",\n",
    "                    },\n",
    "                ],\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 256,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return chat_response.choices[0]\n",
    "\n",
    "    # Method for loading the model\n",
    "    def load_context(self, context):\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    # Method for predicting using the loaded model\n",
    "    def predict(self, model_input):\n",
    "        # Custom prediction logic goes here\n",
    "        pass\n",
    "    \"\"\"\n",
    "    def predict(self, client, message, table_name, spark_object):\n",
    "        table_schema = get_table_schema(table_name, spark_object)\n",
    "        sql_query = self.nl_to_sql(client, message, table_schema)\n",
    "        response = sql_query[\"message\"][\"content\"]\n",
    "        response = response.replace(\"```\", \"\")\n",
    "        #df = spark_object.sql(response)\n",
    "        return response\n",
    "\n",
    "# Create an instance of the custom model\n",
    "defineTxtQueryModel = DefineTxtQuery()\n",
    "\n",
    "# Model Signature to be added to MLlow registration\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"message\"),\n",
    "    ColSpec(\"string\", \"table_name\")\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"sql_query\")\n",
    "])\n",
    "model_signature = ModelSignature(\n",
    "    inputs=input_schema,\n",
    "    outputs=output_schema\n",
    ")\n",
    "\n",
    "# Log the model with MLflow\n",
    "mlflow.pyfunc.log_model(\n",
    "    \"my_custom_model\",\n",
    "    python_model=defineTxtQueryModel,\n",
    "    artifacts={},\n",
    "    signature=model_signature,\n",
    "    registered_model_name=\"workspace.default.define_txt_query_llama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc18c077-6c83-4dbe-ab05-849b4c6ed12e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Testing the Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f89391f-db43-4b17-a72c-9bedcc95c6b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = mlflow.deployments.get_deploy_client(\"databricks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38783175-de21-4224-9bf0-e2afb6be7897",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1st step ask a generic question, an agent will define how to solve this with the data that we have at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33976581-e863-4a4a-89ca-17b7cad5fa9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = defineTxtQueryModel.predict(client, \"How would you describe the housing market for house Sold per city and zip code\", \"bright_data_real_estate_listings.datasets.zillow_properties\", spark)\n",
    "\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7bd188-1f23-4815-b963-c65bf09f0a88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2nd step get you answer transformed into sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528ea461-6570-4ef9-aec2-5cc8bb022ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "result = txtToSQLModel.predict(client, query, \"bright_data_real_estate_listings.datasets.zillow_properties\", spark)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e0da7f-8ee4-4d1e-9523-b75c43fc2c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  CITY, \n",
    "  ZIPCODE, \n",
    "  COUNT(*) AS num_houses_sold, \n",
    "  AVG(PRICE) AS avg_sold_price, \n",
    "  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY PRICE) AS median_sold_price\n",
    "FROM \n",
    "  bright_data_real_estate_listings.datasets.zillow_properties\n",
    "WHERE \n",
    "  HOMESTATUS = 'SOLD'\n",
    "GROUP BY \n",
    "  CITY, \n",
    "  ZIPCODE\n",
    "ORDER BY \n",
    "  CITY, \n",
    "  ZIPCODE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecc8609-f995-4663-85b9-789715774cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3rd step : get the query executed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0eaab6-8744-4bbe-8228-bf8fee46bc8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(result))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1372188875201670,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Txt_to_SQL_with_llama3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
