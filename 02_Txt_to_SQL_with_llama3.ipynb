{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8624fa73-80c4-4e7f-a8b8-c43ff2647df0",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install typing-extensions\n",
    "%pip install openai\n",
    "%pip install sqlparse==0.5.0\n",
    "%pip install mlflow>=2.9.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b38feafd-0ba6-4378-9d45-da2320d7f9b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trying to register to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52805a8-b650-4417-9635-3f5255934ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow[databricks]\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429cd64c-63a6-4d96-8d01-81cc97239543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel, ModelSignature\n",
    "from mlflow.types import DataType, Schema\n",
    "import mlflow.pyfunc\n",
    "import mlflow.deployments\n",
    "import os\n",
    "\n",
    "def get_table_schema(table_name, spark_object):\n",
    "    table_schema = spark_object.sql(\"DESCRIBE {}\".format(table_name))\n",
    "    my_schema = table_schema.collect()\n",
    "    return (table_name, my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be69b332-ea6e-48df-9b23-90d0156b7b12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom model class for TxtToSQL\n",
    "class TxtToSQLModel(PythonModel):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def nl_to_sql(self, client, nl_query, schema):\n",
    "        chat_response = client.predict(\n",
    "            endpoint=\"databricks-meta-llama-3-70b-instruct\",\n",
    "            inputs={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an AI assistant\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Table schema:\\n{schema}\\n\\nConvert the following natural language query to SQL: {nl_query}\\n\\nSQL: and in the output give only the SQL query without text\",\n",
    "                    },\n",
    "                ],\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 256,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return chat_response.choices[0]\n",
    "\n",
    "    # Method for loading the model\n",
    "    def load_context(self, context):\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    # Method for predicting using the loaded model\n",
    "    def predict(self, model_input):\n",
    "        # Custom prediction logic goes here\n",
    "        pass\n",
    "    \"\"\"\n",
    "    def predict(self, client, message, table_name, spark_object):\n",
    "        table_schema = get_table_schema(table_name, spark_object)\n",
    "        sql_query = self.nl_to_sql(client, message, table_schema)\n",
    "        response = sql_query[\"message\"][\"content\"]\n",
    "        response = response.replace(\"```\", \"\")\n",
    "        #df = spark_object.sql(response)\n",
    "        return response\n",
    "\n",
    "# Create an instance of the custom model\n",
    "txtToSQLModel = TxtToSQLModel()\n",
    "\n",
    "# Model Signature to be added to MLlow registration\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"message\"),\n",
    "    ColSpec(\"string\", \"table_name\")\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"sql_query\")\n",
    "])\n",
    "model_signature = ModelSignature(\n",
    "    inputs=input_schema,\n",
    "    outputs=output_schema\n",
    ")\n",
    "\n",
    "# # Log the model with MLflow\n",
    "# mlflow.pyfunc.log_model(\n",
    "#     \"my_custom_model\",\n",
    "#     python_model=txtToSQLModel,\n",
    "#     artifacts={},\n",
    "#     signature=model_signature,\n",
    "#     # registered_model_name=\"workspace.default.txt_to_sql_llama3\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f679ef-e932-4a05-aeec-15aaf74172d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom model class for TxtToSQL\n",
    "class DefineTxtQuery(PythonModel):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def nl_to_sql(self, client, nl_query, schema):\n",
    "        chat_response = client.predict(\n",
    "            endpoint=\"databricks-meta-llama-3-70b-instruct\",\n",
    "            inputs={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an AI assistant\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"From the table having schema :\\n{schema}\\n\\nWhat would you look at to answer this question : {nl_query}\\n\\n Give an answer that contains simple statements looking like natural language queries\",\n",
    "                    },\n",
    "                ],\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 256,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return chat_response.choices[0]\n",
    "\n",
    "    # Method for loading the model\n",
    "    def load_context(self, context):\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    # Method for predicting using the loaded model\n",
    "    def predict(self, model_input):\n",
    "        # Custom prediction logic goes here\n",
    "        pass\n",
    "    \"\"\"\n",
    "    def predict(self, client, message, table_name, spark_object):\n",
    "        table_schema = get_table_schema(table_name, spark_object)\n",
    "        sql_query = self.nl_to_sql(client, message, table_schema)\n",
    "        response = sql_query[\"message\"][\"content\"]\n",
    "        response = response.replace(\"```\", \"\")\n",
    "        #df = spark_object.sql(response)\n",
    "        return response\n",
    "\n",
    "# Create an instance of the custom model\n",
    "defineTxtQueryModel = DefineTxtQuery()\n",
    "\n",
    "# Model Signature to be added to MLlow registration\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"message\"),\n",
    "    ColSpec(\"string\", \"table_name\")\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"sql_query\")\n",
    "])\n",
    "model_signature = ModelSignature(\n",
    "    inputs=input_schema,\n",
    "    outputs=output_schema\n",
    ")\n",
    "\n",
    "# # Log the model with MLflow\n",
    "# mlflow.pyfunc.log_model(\n",
    "#     \"my_custom_model\",\n",
    "#     python_model=defineTxtQueryModel,\n",
    "#     artifacts={},\n",
    "#     signature=model_signature,\n",
    "#     registered_model_name=\"workspace.default.define_txt_query_llama\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc18c077-6c83-4dbe-ab05-849b4c6ed12e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Testing the Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f89391f-db43-4b17-a72c-9bedcc95c6b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = mlflow.deployments.get_deploy_client(\"databricks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38783175-de21-4224-9bf0-e2afb6be7897",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1st step ask a generic question, an agent will define how to solve this with the data that we have at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33976581-e863-4a4a-89ca-17b7cad5fa9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query1 = defineTxtQueryModel.predict(client, \"What is the cheapest place to buy a house past the date timestamp year 2023 for a familiy having more than 2 bathrooms? Always Keep the Latitude and Longitude columns.\", \"workspace.default.us_listings_with_risk_scores\", spark)\n",
    "\n",
    "print(query1)\n",
    "\n",
    "query2 = defineTxtQueryModel.predict(client, \"What are the average sold house prices of the max day in the dataset for each city? Always Keep the Latitude and Longitude columns.\", \"workspace.default.us_listings_with_risk_scores\", spark)\n",
    "\n",
    "print(query2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7bd188-1f23-4815-b963-c65bf09f0a88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2nd step get you answer transformed into sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528ea461-6570-4ef9-aec2-5cc8bb022ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "result1 = txtToSQLModel.predict(client, query1, \"workspace.default.us_listings_with_risk_scores\", spark)\n",
    "\n",
    "print(result1)\n",
    "\n",
    "result2 = txtToSQLModel.predict(client, query2, \"workspace.default.us_listings_with_risk_scores\", spark)\n",
    "\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59326cb2-25dd-40ac-b8b9-591dbebf5949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT * FROM workspace.default.us_listings_with_risk_scores \n",
    "WHERE YEARBUILT > \"2023\" AND BATHROOMS > \"2\" \n",
    "ORDER BY PRICE ASC \n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ecc8609-f995-4663-85b9-789715774cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3rd step : get the query executed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0eaab6-8744-4bbe-8228-bf8fee46bc8d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import plotly.express as px\n",
    "\n",
    "final_df = spark.sql(result1)\n",
    "\n",
    "final_display = spark.sql(\n",
    "    \"\"\"\n",
    "  SELECT\n",
    "  avg(try_cast(LATITUDE AS DOUBLE)) as LATITUDE,\n",
    "  avg(try_cast(LONGITUDE AS DOUBLE)) as LONGITUDE,\n",
    "  avg(PRICE) as PRICE\n",
    "FROM\n",
    "  bright_data_real_estate_listings.datasets.zillow_properties\n",
    "WHERE YEARBUILT > \"2023\" AND BATHROOMS > \"2\" \n",
    "GROUP BY\n",
    "  City\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "final_display = final_display.withColumn(\"PRICE\", col(\"PRICE\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# Example dataframe for plotting\n",
    "df_plot = final_display.toPandas()\n",
    "\n",
    "fig = px.scatter_mapbox(df_plot, lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"PRICE\",\n",
    "                        size=\"PRICE\", color_continuous_scale=px.colors.cyclical.IceFire,\n",
    "                        size_max=20, zoom=2.5, mapbox_style=\"carto-positron\")\n",
    "\n",
    "# Set title\n",
    "fig.update_layout(title=\"Real Estate Prices\")\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    title=\"Real Estate Prices\",\n",
    "    title_x=0.5,  # Center title\n",
    "    title_font_size=24,  # Change title size\n",
    "    title_font_color=\"white\",  # Update title text color\n",
    "    paper_bgcolor=\"black\",  # Set background color\n",
    "    legend_font_color=\"white\",  # Set legend text color to white\n",
    "    font_color=\"white\"  # Set all font color to white\n",
    ")\n",
    "\n",
    "# Rename the display name of a column in the figure\n",
    "fig.update_traces(marker=dict(symbol='circle', opacity=0.8), name=\"Average Price\")\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88908027-b4c0-490f-9b65-bf2310ff906f",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import plotly.express as px\n",
    "\n",
    "final_df_2 = spark.sql(result2)\n",
    "\n",
    "final_display_2 = final_df_2.where(~col(\"LATITUDE\").contains(\"__TYPE__\")).groupBy(\"CITY_NAME\").agg(\n",
    "    F.mean(\"LATITUDE\").cast(\"DOUBLE\").alias(\"LATITUDE\"),\n",
    "    F.mean(\"LONGITUDE\").cast(\"DOUBLE\").alias(\"LONGITUDE\"),\n",
    "    F.mean(\"avg_max_price\").cast(\"DOUBLE\").alias(\"PRICE\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Example dataframe for plotting\n",
    "df_plot = final_display_2.toPandas()\n",
    "\n",
    "fig = px.scatter_mapbox(df_plot, lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"PRICE\",\n",
    "                        size=\"PRICE\", color_continuous_scale=px.colors.cyclical.IceFire,\n",
    "                        size_max=20, zoom=2.5, mapbox_style=\"carto-positron\")\n",
    "\n",
    "# Set title\n",
    "fig.update_layout(title=\"Real Estate Prices\")\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    title=\"Real Estate Prices\",\n",
    "    title_x=0.5,  # Center title\n",
    "    title_font_size=24,  # Change title size\n",
    "    title_font_color=\"white\",  # Update title text color\n",
    "    paper_bgcolor=\"black\",  # Set background color\n",
    "    legend_font_color=\"white\",  # Set legend text color to white\n",
    "    font_color=\"white\"  # Set all font color to white\n",
    ")\n",
    "\n",
    "# Rename the display name of a column in the figure\n",
    "fig.update_traces(marker=dict(symbol='circle', opacity=0.8), name=\"Average Price\")\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1372188875201880,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_Txt_to_SQL_with_llama3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
