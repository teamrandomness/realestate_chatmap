{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb17ce4e-e143-43a9-bd74-c514e34eef9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install torch\n",
    "# %pip install TensorFlow\n",
    "# %pip install flask\n",
    "# %pip install plotly\n",
    "%pip install PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ee8cc9-51ef-45d7-8c3a-f5c7faf5745d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealEstate\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "real_estate_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "      to_date(TIMESTAMP) DATE_DAY,\n",
    "      CITY,\n",
    "      STATE,\n",
    "      CASE WHEN LEN(ZIPCODE) < 5 THEN lpad(ZIPCODE, 5, \"0\") ELSE ZIPCODE END AS ZIPCODE,\n",
    "      HOMESTATUS, \n",
    "      AVG(TRY_CAST(BEDROOMS AS DOUBLE)) AVERAGE_BEDROOMS,\n",
    "      AVG(TRY_CAST(BATHROOMS AS DOUBLE)) AVERAGE_BATHROOMS,\n",
    "      AVG(TRY_CAST(LONGITUDE AS DOUBLE)) LONGITUDE,\n",
    "      AVG(TRY_CAST(LATITUDE AS DOUBLE)) LATITUDE,\n",
    "      AVG(coalesce(CAST(PRICE AS DOUBLE), 0)) AS AVERAGE_PRICE\n",
    "    FROM `bright_data_real_estate_listings`.`datasets`.`zillow_properties`\n",
    "    GROUP BY ALL\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "real_estate_df_clean = real_estate_df.dropna()  # Example: Drop rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fcf165-24ff-40e1-87ab-32b93fbc694b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Example dataframe for plotting\n",
    "df_plot = real_estate_df_clean.toPandas()\n",
    "\n",
    "fig = px.scatter_mapbox(df_plot, lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"AVERAGE_PRICE\",\n",
    "                        size=\"AVERAGE_PRICE\", color_continuous_scale=px.colors.cyclical.IceFire,\n",
    "                        size_max=20, zoom=2.5, mapbox_style=\"carto-positron\")\n",
    "\n",
    "# Set title\n",
    "fig.update_layout(title=\"Real Estate Prices\")\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    title=\"Real Estate Prices\",\n",
    "    title_x=0.5,  # Center title\n",
    "    title_font_size=24,  # Change title size\n",
    "    title_font_color=\"white\",  # Update title text color\n",
    "    paper_bgcolor=\"black\",  # Set background color\n",
    "    legend_font_color=\"white\",  # Set legend text color to white\n",
    "    font_color=\"white\"  # Set all font color to white\n",
    ")\n",
    "\n",
    "# Rename the display name of a column in the figure\n",
    "fig.update_traces(marker=dict(symbol='circle', opacity=0.8), name=\"Average Price\")\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40af29b4-07d4-44c5-8baf-ad1646634886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e76e5a-c920-4904-92c0-53021ccc45e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The EverOrderedLiingo is the dependent variable we wish to determine how it is impacted by the \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = real_estate_df_clean.drop(\"pricePerSquareFoot\")\n",
    "y = real_estate_df_clean.pricePerSquareFoot\n",
    "\n",
    "# Split out the training data\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)\n",
    "\n",
    "# Split the remaining data equally into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0efe3c-1716-4d9c-a447-2e25f4734b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import cloudpickle\n",
    "import time\n",
    "\n",
    "# The predict method of sklearn's RandomForestClassifier returns a binary classification (0 or 1). \n",
    "# The following code creates a wrapper function, SklearnModelWrapper, that uses \n",
    "# the predict_proba method to return the probability that the observation belongs to each class. \n",
    "\n",
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def predict(self, context, model_input):\n",
    "    return self.model.predict_proba(model_input)[:,1]\n",
    "\n",
    "# mlflow.start_run creates a new MLflow run to track the performance of this model. \n",
    "# Within the context, you call mlflow.log_param to keep track of the parameters used, and\n",
    "# mlflow.log_metric to record metrics like accuracy.\n",
    "with mlflow.start_run(run_name='untuned_random_forest'):\n",
    "  n_estimators = 10\n",
    "  model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(123))\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "  predictions_test = model.predict_proba(X_test)[:,1]\n",
    "  auc_score = roc_auc_score(y_test, predictions_test)\n",
    "  mlflow.log_param('n_estimators', n_estimators)\n",
    "  # Use the area under the ROC curve as a metric.\n",
    "  mlflow.log_metric('auc', auc_score)\n",
    "  wrappedModel = SklearnModelWrapper(model)\n",
    "  # Log the model with a signature that defines the schema of the model's inputs and outputs. \n",
    "  # When the model is deployed, this signature will be used to validate inputs.\n",
    "  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n",
    "  \n",
    "  # MLflow contains utilities to create a conda environment used to serve models.\n",
    "  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.\n",
    "  conda_env =  _mlflow_conda_env(\n",
    "        additional_conda_deps=None,\n",
    "        additional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.__version__), \"scikit-learn=={}\".format(sklearn.__version__)],\n",
    "        additional_conda_channels=None,\n",
    "    )\n",
    "  mlflow.pyfunc.log_model(\"random_forest_model\", python_model=wrappedModel, conda_env=conda_env, signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbb9659-6a3e-4d1c-b273-ce4824962ad0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns.tolist(), columns=['importance'])\n",
    "feature_importances.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a756809-11bd-45ca-a350-dd2b8b98bea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1372188875201645,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Visualiaze_On_Map",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
